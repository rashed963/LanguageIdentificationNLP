{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rashed963/LanguageIdentificationNLP/blob/main/notebooks/LR_Word2Vec%26GridSearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "m6WTnuuBfhDW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline, Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from joblib import dump\n",
        "from gensim.models.callbacks import CallbackAny2Vec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure necessary NLTK components are downloaded\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfwJgFC5PEgc",
        "outputId": "56ca782b-0c91-4a3d-f03e-c68ffba22202"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3CMYVBTflZf",
        "outputId": "1947fecb-ee85-49ca-b0f7-688c5789e9e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data preprocessing"
      ],
      "metadata": {
        "id": "y1IBS0NBUF4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess the input text for language identification.\n",
        "\n",
        "    Args:\n",
        "    text (str): The input text to preprocess.\n",
        "\n",
        "    Returns:\n",
        "    str: The preprocessed text.\n",
        "    \"\"\"\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)  # Keep lowercased letters and spaces\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "HB9T0yeYULvX"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loading and Sampling\n",
        "\n",
        "This section covers the loading and random sampling of the training and testing data. Sampling is particularly useful to reduce computation time while developing the model on Google Colab. We use a sample size of 25,000 for training and 10,000 for testing to ensure a representative subset of the full dataset."
      ],
      "metadata": {
        "id": "D8lJBQbDUgGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_sample(filepath, sample_size=25000, random_state=42):\n",
        "      \"\"\"Load a random sample of lines from a file to reduce memory usage and speed up computations.\"\"\"\n",
        "\n",
        "      with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "      random.seed(random_state)\n",
        "      sampled_indices = random.sample(range(len(lines)), sample_size)\n",
        "      sampled_lines = [lines[i].strip() for i in sampled_indices]\n",
        "      return sampled_lines\n",
        "\n",
        "def load_datasets(train_data_path, train_labels_path, test_data_path, test_labels_path, train_sample_size=25000, test_sample_size=10000):\n",
        "    \"\"\"\n",
        "    Load training and testing data and labels from specified file paths.\n",
        "\n",
        "    Args:\n",
        "    train_data_path (str): Path to the training data file.\n",
        "    train_labels_path (str): Path to the training labels file.\n",
        "    test_data_path (str): Path to the testing data file.\n",
        "    test_labels_path (str): Path to the testing labels file.\n",
        "    train_sample_size (int): Number of samples to load from the training data.\n",
        "    test_sample_size (int): Number of samples to load from the testing data.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Tuple containing loaded training data, training labels, testing data, testing labels.\n",
        "    \"\"\"\n",
        "    # Load training data and labels\n",
        "    X_train = load_data_sample(train_data_path, sample_size=train_sample_size)\n",
        "    y_train = load_data_sample(train_labels_path, sample_size=train_sample_size)\n",
        "\n",
        "    # Load testing data and labels\n",
        "    X_test = load_data_sample(test_data_path, sample_size=test_sample_size)\n",
        "    y_test = load_data_sample(test_labels_path, sample_size=test_sample_size)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "# Define the base path for data files\n",
        "base_path = '/content/drive/MyDrive/data'\n",
        "\n",
        "# Paths to data files using the base path\n",
        "train_data_path = f'{base_path}/train/x_train.txt'\n",
        "train_labels_path = f'{base_path}/train/y_train.txt'\n",
        "test_data_path = f'{base_path}/test/x_test.txt'\n",
        "test_labels_path = f'{base_path}/test/y_test.txt'"
      ],
      "metadata": {
        "id": "GQTyAk5FUm90"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_test, y_test = load_datasets(train_data_path, train_labels_path, test_data_path, test_labels_path)"
      ],
      "metadata": {
        "id": "y6yC0vOlU9BA"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cbmY0Tfsfn2O"
      },
      "outputs": [],
      "source": [
        "# def load_data_sample(filepath, sample_size=10, random_state=42):\n",
        "#     with open(filepath, 'r', encoding='utf-8') as file:\n",
        "#         lines = file.readlines()\n",
        "\n",
        "#     random.seed(random_state)\n",
        "#     sampled_indices = random.sample(range(len(lines)), sample_size)\n",
        "#     sampled_lines = [lines[i].strip() for i in sampled_indices]\n",
        "#     return sampled_lines\n",
        "\n",
        "# def train_model(X_train, y_train):\n",
        "#     model = make_pipeline(CountVectorizer(analyzer='char', ngram_range=(2, 2)), LogisticRegression(max_iter=500))\n",
        "#     model.fit(X_train, y_train)\n",
        "#     return model\n",
        "\n",
        "# def predict_language(text, model):\n",
        "#     return model.predict([text])[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HQFLqkQMg55u"
      },
      "outputs": [],
      "source": [
        "# filepath = '/content/drive/MyDrive/data/train/x_train.txt'\n",
        "# X_train = load_data_sample(filepath,sample_size=25000)\n",
        "# filepath = '/content/drive/MyDrive/data/train/y_train.txt'\n",
        "# y_train = load_data_sample(filepath,sample_size=25000)\n",
        "# filepath = '/content/drive/MyDrive/data/test/x_test.txt'\n",
        "# X_test = load_data_sample(filepath,sample_size=10000)\n",
        "# filepath = '/content/drive/MyDrive/data/test/y_test.txt'\n",
        "# y_test = load_data_sample(filepath,sample_size=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec Model Training and Transformation\n",
        "\n",
        "This section defines the Word2Vec transformation and training process. We utilize the `Word2Vec` model from the Gensim library, which is effective for natural language processing tasks such as embedding generation based on the context of words.\n",
        "\n",
        "### EpochLogger Callback\n",
        "To monitor the training progress of the Word2Vec model, we implement the `EpochLogger` class as a callback. This callback logs the completion of each epoch during training, providing visibility into the model's training process. This is especially useful for long training sessions, as it gives real-time feedback about the progress.\n",
        "\n",
        "### Word2Vec Transformation Function\n",
        "The `word2vec_transform` function is responsible for:\n",
        "1. **Initializing and training the Word2Vec model**: The function takes sentences as input and trains a Word2Vec model. Key parameters such as `vector_size`, `window`, and `min_count` are configurable, allowing customization based on specific dataset characteristics.\n",
        "2. **Generating word embeddings**: After training, the model computes the average Word2Vec embedding for each sentence. This average embedding represents the sentence in a dense vector form, which can be used for further machine learning tasks.\n",
        "\n",
        "This approach leverages the semantic richness of Word2Vec embeddings, providing a robust feature set for subsequent classification or clustering tasks.\n"
      ],
      "metadata": {
        "id": "_qc4fBh_W7kf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "RRdJ2yLdyZBx"
      },
      "outputs": [],
      "source": [
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "\n",
        "# Callback to print loss after each epoch\n",
        "class EpochLogger(CallbackAny2Vec):\n",
        "    def __init__(self):\n",
        "        self.epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, model):\n",
        "        print(f\"Epoch {self.epoch} completed\")\n",
        "        self.epoch += 1\n",
        "\n",
        "def word2vec_transform(sentences, vector_size=100, window=5, min_count=1, epochs=10):\n",
        "    epoch_logger = EpochLogger()  # Initialize the logger\n",
        "    model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, callbacks=[epoch_logger], epochs=epochs)\n",
        "    word_vectors = model.wv\n",
        "    return np.array([\n",
        "        np.mean([word_vectors[w] for w in words if w in word_vectors.key_to_index] or [np.zeros(vector_size)], axis=0)\n",
        "        for words in sentences\n",
        "    ])\n",
        "# from gensim.models import Word2Vec\n",
        "# from gensim.models.callbacks import CallbackAny2Vec\n",
        "\n",
        "# class EpochLogger(CallbackAny2Vec):\n",
        "#     def __init__(self):\n",
        "#         self.epoch = 0\n",
        "#     def on_epoch_end(self, model):\n",
        "#         print(f\"Epoch {self.epoch} completed\")\n",
        "#         self.epoch += 1\n",
        "\n",
        "# def word2vec_transform(sentences, vector_size=100, window=5, min_count=1, epochs=10):\n",
        "#     epoch_logger = EpochLogger()\n",
        "#     model = Word2Vec(vector_size=vector_size, window=window, min_count=min_count)\n",
        "#     model.build_vocab(sentences)  # Build vocabulary from the sentences\n",
        "#     model.train(sentences, total_examples=model.corpus_count, epochs=epochs, callbacks=[epoch_logger])\n",
        "#     word_vectors = model.wv\n",
        "#     return np.array([\n",
        "#         np.mean([word_vectors[w] for w in words if w in word_vectors.key_to_index] or [np.zeros(vector_size)], axis=0)\n",
        "#         for words in sentences\n",
        "#     ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD3mAKSMhAS4"
      },
      "source": [
        "## Prediction and Model Training Functions\n",
        "\n",
        "This section outlines the core functionalities of our language identification pipeline, covering the model training, prediction, and evaluation processes. These functions are designed to work with different types of text vectorization techniques, namely TF-IDF and Word2Vec.\n",
        "\n",
        "### Predict Language Function\n",
        "The `predict_language` function handles the prediction of the language for a given piece of text based on the specified model and vectorizer type:\n",
        "- **TF-IDF Vectorization**: If the model is trained with TF-IDF, the prediction is straightforwardâ€”passing the text through the pipeline to get the predicted language.\n",
        "- **Word2Vec Vectorization**: For Word2Vec, the text needs to be tokenized and transformed into Word2Vec embeddings before making a prediction. This involves converting the text into tokens, transforming these tokens into embeddings, and then reshaping the result to match the expected input structure for the classifier.\n",
        "\n",
        "### Train and Tune Model Function\n",
        "The `train_and_tune_model` function configures and trains the model based on the specified vectorization technique:\n",
        "- **TF-IDF**: A pipeline is set up with a TF-IDF vectorizer and a logistic regression classifier. Hyperparameters are tuned using GridSearchCV to find the best model configuration.\n",
        "- **Word2Vec**: The training data are first transformed into Word2Vec embeddings, followed by training a logistic regression classifier. This function outputs the trained model ready for making predictions.\n",
        "\n",
        "### Predict and Evaluate Function\n",
        "The `predict_and_evaluate` function is used to assess the performance of the trained model. It iterates over a dataset, applies the `predict_language` function to each text entry, and collects predictions. It then calculates and prints the accuracy of the model based on these predictions, providing a straightforward metric to evaluate the model's effectiveness.\n",
        "\n",
        "This setup allows for a flexible application of different text vectorization techniques, facilitating easy comparison and selection based on performance metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "1KZAwSByg79F"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline, Pipeline\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "def predict_language(text, model, vectorizer_type='tfidf'):\n",
        "    if vectorizer_type == 'tfidf':\n",
        "        # For TF-IDF, predict directly using the model\n",
        "        return model.predict([text])[0]\n",
        "    elif vectorizer_type == 'word2vec':\n",
        "        # For Word2Vec, transform the text first\n",
        "        # Tokenize the single text\n",
        "        tokenized_text = word_tokenize(text)\n",
        "        # Transform the text using Word2Vec\n",
        "        transformed_text = word2vec_transform([tokenized_text])\n",
        "        # Reshape the input to (1, -1), which is (1 sample, N features)\n",
        "        transformed_text = transformed_text.reshape(1, -1)\n",
        "        return model.predict(transformed_text)[0]\n",
        "\n",
        "# Function to train Word2Vec and transform data\n",
        "def word2vec_transform(sentences, vector_size=100, window=5, min_count=1, epochs=10):\n",
        "    # Initialize and train a Word2Vec model\n",
        "    model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, epochs=epochs)\n",
        "    word_vectors = model.wv\n",
        "    # Compute the average of word vectors for each sentence\n",
        "    return np.array([\n",
        "        np.mean([word_vectors[w] for w in words if w in word_vectors.key_to_index] or [np.zeros(vector_size)], axis=0)\n",
        "        for words in sentences\n",
        "    ])\n",
        "\n",
        "# Define the model and hyperparameters\n",
        "def train_and_tune_model(X_train, y_train, vectorizer_type='tfidf'):\n",
        "    if vectorizer_type == 'tfidf':\n",
        "        vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(3, 3))\n",
        "        pipeline = make_pipeline(vectorizer, LogisticRegression(max_iter=500))\n",
        "    elif vectorizer_type == 'word2vec':\n",
        "        print(\"Transforming training data using Word2Vec...\")\n",
        "        X_train_transformed = word2vec_transform(X_train)\n",
        "        pipeline = Pipeline([('classifier', LogisticRegression())])\n",
        "        print(\"Fitting Word2Vec model...\")\n",
        "        pipeline.fit(X_train_transformed, y_train)\n",
        "        return pipeline\n",
        "\n",
        "    parameters = {'logisticregression__C': [0.1, 1]}\n",
        "    if vectorizer_type == 'tfidf':\n",
        "        parameters['tfidfvectorizer__ngram_range'] = [(2, 2)]\n",
        "        print(\"Starting grid search for TF-IDF...\")\n",
        "    grid_search = GridSearchCV(pipeline, parameters, cv=5, verbose=2)  # Increase verbose for detailed logging\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    print(\"Best model selected.\")\n",
        "    return grid_search.best_estimator_\n",
        "\n",
        "def predict_and_evaluate(X, y, model, vectorizer_type):\n",
        "    predictions = []\n",
        "    total = len(X)\n",
        "    print(f\"Starting predictions for {vectorizer_type}...\")\n",
        "    for index, text in enumerate(X):\n",
        "        prediction = predict_language(text, model, vectorizer_type)\n",
        "        predictions.append(prediction)\n",
        "        if (index + 1) % 100 == 0 or index + 1 == total:  # Update every 100 samples or last sample\n",
        "            print(f\"Completed {index + 1}/{total} predictions\")\n",
        "    accuracy = accuracy_score(y, predictions)\n",
        "    print(f\"{vectorizer_type} Accuracy: {accuracy}\")\n",
        "    return accuracy\n",
        "\n",
        "# def predict_language(text, model, vectorizer_type='tfidf'):\n",
        "#     if vectorizer_type == 'tfidf':\n",
        "#         return model.predict([text])[0]\n",
        "#     elif vectorizer_type == 'word2vec':\n",
        "#         tokenized_text = word_tokenize(text)\n",
        "#         transformed_text = word2vec_transform([tokenized_text])\n",
        "#         transformed_text = transformed_text.reshape(1, -1)\n",
        "#         return model.predict(transformed_text)[0]\n",
        "\n",
        "# def train_and_tune_model(X_train, y_train, vectorizer_type='tfidf'):\n",
        "#     if vectorizer_type == 'tfidf':\n",
        "#         vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(3, 3))\n",
        "#         pipeline = make_pipeline(vectorizer, LogisticRegression(max_iter=500))\n",
        "#         parameters = {'logisticregression__C': [0.1, 1], 'tfidfvectorizer__ngram_range': [(2, 2)]}\n",
        "#         grid_search = GridSearchCV(pipeline, parameters, cv=5, verbose=2)\n",
        "#         grid_search.fit(X_train, y_train)\n",
        "#         print(\"Best TF-IDF model selected.\")\n",
        "#         return grid_search.best_estimator_\n",
        "#     elif vectorizer_type == 'word2vec':\n",
        "#         print(\"Transforming training data using Word2Vec...\")\n",
        "#         X_train_transformed = word2vec_transform(X_train)\n",
        "#         pipeline = Pipeline([('classifier', LogisticRegression(max_iter=500))])\n",
        "#         pipeline.fit(X_train_transformed, y_train)\n",
        "#         print(\"Word2Vec model fitted.\")\n",
        "#         return pipeline\n",
        "\n",
        "# def predict_and_evaluate(X, y, model, vectorizer_type):\n",
        "#     predictions = []\n",
        "#     for index, text in enumerate(X):\n",
        "#         predictions.append(predict_language(text, model, vectorizer_type))\n",
        "#         if (index + 1) % 100 == 0 or index + 1 == len(X):\n",
        "#             print(f\"Completed {index + 1}/{len(X)} predictions\")\n",
        "#     accuracy = accuracy_score(y, predictions)\n",
        "#     print(f\"{vectorizer_type} Accuracy: {accuracy}\")\n",
        "#     return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training, Evaluating, and Saving the TF-IDF Model\n",
        "\n",
        "This section is dedicated to handling the operations for the TF-IDF vectorized model. We follow a structured approach to train the model, evaluate its performance, and then save it for future use. This process ensures that we have a deployable model at hand without the need to retrain.\n",
        "\n",
        "### Training the Model\n",
        "We start by training our model using the `train_and_tune_model` function with the TF-IDF vectorization approach. This function not only trains the model but also tunes its hyperparameters using GridSearchCV to find the optimal model settings. This ensures that our model performs at its best.\n",
        "\n",
        "### Evaluating the Model\n",
        "Once the model is trained, we use the `predict_and_evaluate` function to test the model's performance on a separate test dataset. This function predicts the languages of the texts in the test dataset and calculates the accuracy, providing a quantitative measure of the model's effectiveness.\n",
        "\n",
        "### Saving the Model\n",
        "After confirming the model's performance, we save the trained model to disk using `joblib`. This step is crucial as it allows us to reuse the trained model without undergoing the training process again, saving both time and computational resources. The model is stored in the 'models' directory under the name `model_tfidf.joblib`.\n",
        "\n",
        "This end-to-end process from training to saving ensures that our model is not only accurate but also readily available for deployment or further experimentation.\n"
      ],
      "metadata": {
        "id": "Dqp84w25XoAu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD4wpGFnyy-Z",
        "outputId": "40be7823-687c-4252-e90b-ee58e3220639"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting grid search for TF-IDF...\n",
            "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
            "[CV] END logisticregression__C=0.1, tfidfvectorizer__ngram_range=(2, 2); total time= 3.4min\n",
            "[CV] END logisticregression__C=0.1, tfidfvectorizer__ngram_range=(2, 2); total time= 3.2min\n",
            "[CV] END logisticregression__C=0.1, tfidfvectorizer__ngram_range=(2, 2); total time= 3.0min\n",
            "[CV] END logisticregression__C=0.1, tfidfvectorizer__ngram_range=(2, 2); total time= 3.2min\n",
            "[CV] END logisticregression__C=0.1, tfidfvectorizer__ngram_range=(2, 2); total time= 3.1min\n",
            "[CV] END logisticregression__C=1, tfidfvectorizer__ngram_range=(2, 2); total time= 7.3min\n",
            "[CV] END logisticregression__C=1, tfidfvectorizer__ngram_range=(2, 2); total time= 7.3min\n",
            "[CV] END logisticregression__C=1, tfidfvectorizer__ngram_range=(2, 2); total time= 7.3min\n",
            "[CV] END logisticregression__C=1, tfidfvectorizer__ngram_range=(2, 2); total time= 6.9min\n",
            "[CV] END logisticregression__C=1, tfidfvectorizer__ngram_range=(2, 2); total time= 7.1min\n",
            "Best model selected.\n",
            "Starting predictions for tfidf...\n",
            "Completed 100/10000 predictions\n",
            "Completed 200/10000 predictions\n",
            "Completed 300/10000 predictions\n",
            "Completed 400/10000 predictions\n",
            "Completed 500/10000 predictions\n",
            "Completed 600/10000 predictions\n",
            "Completed 700/10000 predictions\n",
            "Completed 800/10000 predictions\n",
            "Completed 900/10000 predictions\n",
            "Completed 1000/10000 predictions\n",
            "Completed 1100/10000 predictions\n",
            "Completed 1200/10000 predictions\n",
            "Completed 1300/10000 predictions\n",
            "Completed 1400/10000 predictions\n",
            "Completed 1500/10000 predictions\n",
            "Completed 1600/10000 predictions\n",
            "Completed 1700/10000 predictions\n",
            "Completed 1800/10000 predictions\n",
            "Completed 1900/10000 predictions\n",
            "Completed 2100/10000 predictions\n",
            "Completed 2200/10000 predictions\n",
            "Completed 2300/10000 predictions\n",
            "Completed 2400/10000 predictions\n",
            "Completed 2500/10000 predictions\n",
            "Completed 2600/10000 predictions\n",
            "Completed 2700/10000 predictions\n",
            "Completed 2800/10000 predictions\n",
            "Completed 2900/10000 predictions\n",
            "Completed 3000/10000 predictions\n",
            "Completed 3100/10000 predictions\n",
            "Completed 3200/10000 predictions\n",
            "Completed 3300/10000 predictions\n",
            "Completed 3400/10000 predictions\n",
            "Completed 3500/10000 predictions\n",
            "Completed 3600/10000 predictions\n",
            "Completed 3700/10000 predictions\n",
            "Completed 3800/10000 predictions\n",
            "Completed 3900/10000 predictions\n",
            "Completed 4000/10000 predictions\n",
            "Completed 4100/10000 predictions\n",
            "Completed 4200/10000 predictions\n",
            "Completed 4300/10000 predictions\n",
            "Completed 4400/10000 predictions\n",
            "Completed 4500/10000 predictions\n",
            "Completed 4600/10000 predictions\n",
            "Completed 4700/10000 predictions\n",
            "Completed 4800/10000 predictions\n",
            "Completed 4900/10000 predictions\n",
            "Completed 5000/10000 predictions\n",
            "Completed 5100/10000 predictions\n",
            "Completed 5200/10000 predictions\n",
            "Completed 5300/10000 predictions\n",
            "Completed 5400/10000 predictions\n",
            "Completed 5500/10000 predictions\n",
            "Completed 5600/10000 predictions\n",
            "Completed 5700/10000 predictions\n",
            "Completed 5800/10000 predictions\n",
            "Completed 5900/10000 predictions\n",
            "Completed 6000/10000 predictions\n",
            "Completed 6100/10000 predictions\n",
            "Completed 6200/10000 predictions\n",
            "Completed 6300/10000 predictions\n",
            "Completed 6400/10000 predictions\n",
            "Completed 6500/10000 predictions\n",
            "Completed 6600/10000 predictions\n",
            "Completed 6700/10000 predictions\n",
            "Completed 6800/10000 predictions\n",
            "Completed 6900/10000 predictions\n",
            "Completed 7000/10000 predictions\n",
            "Completed 7100/10000 predictions\n",
            "Completed 7200/10000 predictions\n",
            "Completed 7300/10000 predictions\n",
            "Completed 7400/10000 predictions\n",
            "Completed 7500/10000 predictions\n",
            "Completed 7600/10000 predictions\n",
            "Completed 7700/10000 predictions\n",
            "Completed 7800/10000 predictions\n",
            "Completed 7900/10000 predictions\n",
            "Completed 8000/10000 predictions\n",
            "Completed 8100/10000 predictions\n",
            "Completed 8200/10000 predictions\n",
            "Completed 8300/10000 predictions\n",
            "Completed 8400/10000 predictions\n",
            "Completed 8500/10000 predictions\n",
            "Completed 8600/10000 predictions\n",
            "Completed 8700/10000 predictions\n",
            "Completed 8800/10000 predictions\n",
            "Completed 8900/10000 predictions\n",
            "Completed 9000/10000 predictions\n",
            "Completed 9100/10000 predictions\n",
            "Completed 9200/10000 predictions\n",
            "Completed 9300/10000 predictions\n",
            "Completed 9400/10000 predictions\n",
            "Completed 9500/10000 predictions\n",
            "Completed 9600/10000 predictions\n",
            "Completed 9700/10000 predictions\n",
            "Completed 9800/10000 predictions\n",
            "Completed 9900/10000 predictions\n",
            "Completed 10000/10000 predictions\n",
            "tfidf Accuracy: 0.9274\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['model_tfidf.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "model_tfidf = train_and_tune_model(X_train, y_train, vectorizer_type='tfidf')\n",
        "y_pred_tfidf = predict_and_evaluate(X_test, y_test, model_tfidf, 'tfidf')\n",
        "dump(model_tfidf, 'model_tfidf.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('model_tfidf.joblib')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "khPENtEu8dvq",
        "outputId": "ed6d9ac4-6f37-46d9-8a33-a9ffddfabc4b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_892718cb-304e-4ec8-8581-46372e27d274\", \"model_tfidf.joblib\", 240440625)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iR7wker3hItv",
        "outputId": "99023249-ad42-4aac-f686-18dcf3de8d3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline(steps=[('tfidfvectorizer',\n",
            "                 TfidfVectorizer(analyzer='char', ngram_range=(2, 2))),\n",
            "                ('logisticregression', LogisticRegression(C=1, max_iter=500))])\n"
          ]
        }
      ],
      "source": [
        "from joblib import dump, load\n",
        "loaded_model_tfidf = load('model_tfidf.joblib')\n",
        "print(loaded_model_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training, Evaluating, and Saving the Word2Vec Model\n",
        "\n",
        "This section focuses on the procedures using the Word2Vec model. We demonstrate the comprehensive steps from training the model with Word2Vec embeddings, evaluating its performance on the test data, and finally saving the model for later use. These steps ensure our model is both effective and reusable.\n",
        "\n",
        "### Training the Word2Vec Model\n",
        "The `train_and_tune_model` function is utilized here with the `word2vec` vectorization type. Unlike traditional TF-IDF, Word2Vec provides a dense representation of words which captures semantic relationships. This function trains the model using these embeddings and integrates them into a logistic regression framework to predict the languages. It's designed to offer a deep understanding of context within the text data.\n",
        "\n",
        "### Evaluating the Model\n",
        "After training, the performance of the Word2Vec model is evaluated using the `predict_and_evaluate` function. This function applies the trained model to the test dataset to predict languages, and calculates the accuracy to quantify how well our model performs in real-world scenarios. This step is crucial for validating the effectiveness of the Word2Vec embeddings in the task of language identification.\n",
        "\n",
        "### Saving the Model\n",
        "Once we verify the model's performance and are satisfied with the results, the final step is to save this model using `joblib`. Saving the model to the 'models' directory as `model_word2vec.joblib` allows us to deploy or further experiment with the model without the need for retraining, ensuring efficiency and readiness for practical applications.\n",
        "\n",
        "By following these structured steps, we ensure that our Word2Vec model is not only trained to capture the nuances of different languages but also ready for immediate deployment or further development.\n"
      ],
      "metadata": {
        "id": "e6lEcIPlX4qG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I4IumQZ2zUUf",
        "outputId": "9cc1297d-1073-46f8-f991-ee43ae144912"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transforming training data using Word2Vec...\n",
            "Fitting Word2Vec model...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting predictions for word2vec...\n",
            "Completed 100/10000 predictions\n",
            "Completed 200/10000 predictions\n",
            "Completed 300/10000 predictions\n",
            "Completed 400/10000 predictions\n",
            "Completed 500/10000 predictions\n",
            "Completed 600/10000 predictions\n",
            "Completed 700/10000 predictions\n",
            "Completed 800/10000 predictions\n",
            "Completed 900/10000 predictions\n",
            "Completed 1000/10000 predictions\n",
            "Completed 1100/10000 predictions\n",
            "Completed 1200/10000 predictions\n",
            "Completed 1300/10000 predictions\n",
            "Completed 1400/10000 predictions\n",
            "Completed 1500/10000 predictions\n",
            "Completed 1600/10000 predictions\n",
            "Completed 1700/10000 predictions\n",
            "Completed 1800/10000 predictions\n",
            "Completed 1900/10000 predictions\n",
            "Completed 2000/10000 predictions\n",
            "Completed 2100/10000 predictions\n",
            "Completed 2200/10000 predictions\n",
            "Completed 2300/10000 predictions\n",
            "Completed 2400/10000 predictions\n",
            "Completed 2500/10000 predictions\n",
            "Completed 2600/10000 predictions\n",
            "Completed 2700/10000 predictions\n",
            "Completed 2800/10000 predictions\n",
            "Completed 2900/10000 predictions\n",
            "Completed 3000/10000 predictions\n",
            "Completed 3100/10000 predictions\n",
            "Completed 3200/10000 predictions\n",
            "Completed 3300/10000 predictions\n",
            "Completed 3400/10000 predictions\n",
            "Completed 3500/10000 predictions\n",
            "Completed 3600/10000 predictions\n",
            "Completed 3700/10000 predictions\n",
            "Completed 3800/10000 predictions\n",
            "Completed 3900/10000 predictions\n",
            "Completed 4000/10000 predictions\n",
            "Completed 4100/10000 predictions\n",
            "Completed 4200/10000 predictions\n",
            "Completed 4300/10000 predictions\n",
            "Completed 4400/10000 predictions\n",
            "Completed 4500/10000 predictions\n",
            "Completed 4600/10000 predictions\n",
            "Completed 4700/10000 predictions\n",
            "Completed 4800/10000 predictions\n",
            "Completed 4900/10000 predictions\n",
            "Completed 5000/10000 predictions\n",
            "Completed 5100/10000 predictions\n",
            "Completed 5200/10000 predictions\n",
            "Completed 5300/10000 predictions\n",
            "Completed 5400/10000 predictions\n",
            "Completed 5500/10000 predictions\n",
            "Completed 5600/10000 predictions\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-feb4f228d7a3>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_word2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_tune_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'word2vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my_pred_word2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_word2vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'word2vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# dump(model_word2vec, 'model_word2vec.joblib')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-835a08f2884d>\u001b[0m in \u001b[0;36mpredict_and_evaluate\u001b[0;34m(X, y, model, vectorizer_type)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting predictions for {vectorizer_type}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_language\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Update every 100 samples or last sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-835a08f2884d>\u001b[0m in \u001b[0;36mpredict_language\u001b[0;34m(text, model, vectorizer_type)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Transform the text using Word2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtransformed_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Reshape the input to (1, -1), which is (1 sample, N features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtransformed_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformed_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-835a08f2884d>\u001b[0m in \u001b[0;36mword2vec_transform\u001b[0;34m(sentences, vector_size, window, min_count, epochs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mword2vec_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Initialize and train a Word2Vec model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Compute the average of word vectors for each sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             self.train(\n\u001b[0m\u001b[1;32m    431\u001b[0m                 \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcorpus_iterable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[0m\u001b[1;32m   1074\u001b[0m                     \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m                     \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[1;32m   1430\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdaemon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# make interrupting the process with ctrl+c easier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1432\u001b[0;31m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    938\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0m_limbo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_started\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Call the modified function for predictions and evaluations\n",
        "\n",
        "model_word2vec = train_and_tune_model(X_train, y_train, vectorizer_type='word2vec')\n",
        "y_pred_word2vec = predict_and_evaluate(X_test, y_test, model_word2vec, 'word2vec')\n",
        "# dump(model_word2vec, 'model_word2vec.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('model_word2vec.joblib')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "jdSljs1kM6mt",
        "outputId": "a420eca0-ec0b-4ac3-8a49-c3e1c55981a7"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ef54d4db-6c46-4182-a9c8-d3aad90c9bed\", \"model_word2vec.joblib\", 199253)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'example'\n",
        "if word in model_word2vec.named_steps['word2vec'].model:\n",
        "    print(f\"Vector for '{word}':\", model_word2vec.wv[word])\n",
        "else:\n",
        "    print(f\"Word '{word}' not in vocabulary.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Hu8lX8mMgXV3",
        "outputId": "1c28385b-1080-4737-923a-692751508f73"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'word2vec'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-a701d086a4ab>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'example'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_word2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word2vec'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Vector for '{word}':\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_word2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Word '{word}' not in vocabulary.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'word2vec'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ej3wKXXakbZ3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPd8qTHP1eLgHT9AGSADNQd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}