{"cells":[{"cell_type":"markdown","source":["## Introduction to Approach 2: TF-IDF and Word2Vec with Grid Search Optimization\n","\n","In this notebook, we explore two advanced text vectorization techniques, TF-IDF (Term Frequency-Inverse Document Frequency) and Word2Vec, combined with logistic regression to develop a robust language identification model. Each method offers unique advantages in processing and classifying textual data across various languages.\n","\n","### TF-IDF Vectorization\n","TF-IDF is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. Here, we use a character-level TF-IDF representation, which effectively captures the linguistic patterns at the character level, making it particularly useful for language identification tasks where such granular features can significantly distinguish between languages.\n","\n","### Word2Vec Embeddings\n","Conversely, Word2Vec provides a dense vector representation of words, capturing the context within which words appear. This model is trained using the surrounding words and thus, tends to capture semantic relationships more effectively than TF-IDF.\n","\n","### Grid Search for Hyperparameter Optimization\n","To fine-tune our logistic regression models trained with these vectorizations, we employ Grid Search CV. This method exhaustively searches through a specified parameter space, allowing us to identify the optimal settings for our models and ensure the best possible performance.\n","\n","\n","\n","### References\n","\n","1. Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. *Proceedings of Workshop at ICLR*. Available at [arXiv:1301.3781](https://arxiv.org/abs/1301.3781)."],"metadata":{"id":"22z-u3eHkqte"}},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":300,"status":"ok","timestamp":1714920313242,"user":{"displayName":"Rashed Sabra","userId":"05362189444872837845"},"user_tz":-120},"id":"m6WTnuuBfhDW"},"outputs":[],"source":["import numpy as np\n","import random\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import make_pipeline, Pipeline\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import accuracy_score\n","from gensim.models import Word2Vec\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from joblib import dump\n","from gensim.models.callbacks import CallbackAny2Vec"]},{"cell_type":"code","source":["# Ensure necessary NLTK components are downloaded\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CfwJgFC5PEgc","executionInfo":{"status":"ok","timestamp":1714920314308,"user_tz":-120,"elapsed":351,"user":{"displayName":"Rashed Sabra","userId":"05362189444872837845"}},"outputId":"5058eb55-16d9-4179-daa6-463a7d6c5cbd"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1909,"status":"ok","timestamp":1714920317881,"user":{"displayName":"Rashed Sabra","userId":"05362189444872837845"},"user_tz":-120},"id":"G3CMYVBTflZf","outputId":"ac34563b-0bb6-4125-9d19-5d0b364b63ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["#Data preprocessing"],"metadata":{"id":"y1IBS0NBUF4j"}},{"cell_type":"code","source":["import re\n","#not used for now\n","def preprocess_text(text):\n","    \"\"\"\n","    Preprocess the input text for language identification.\n","\n","    Args:\n","    text (str): The input text to preprocess.\n","\n","    Returns:\n","    str: The preprocessed text.\n","    \"\"\"\n","    return text"],"metadata":{"id":"HB9T0yeYULvX","executionInfo":{"status":"ok","timestamp":1714906207370,"user_tz":-120,"elapsed":283,"user":{"displayName":"Rashed Sabra","userId":"05362189444872837845"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["#Data Loading and Sampling\n","\n","This section covers the loading and random sampling of the training and testing data. Sampling is particularly useful to reduce computation time while developing the model on Google Colab. We use a sample size of 25,000 for training and 10,000 for testing to ensure a representative subset of the full dataset."],"metadata":{"id":"D8lJBQbDUgGe"}},{"cell_type":"code","source":["def load_data_sample(filepath, sample_size=25000, random_state=42):\n","      \"\"\"Load a random sample of lines from a file to reduce memory usage and speed up computations.\"\"\"\n","\n","      with open(filepath, 'r', encoding='utf-8') as file:\n","        lines = file.readlines()\n","\n","      random.seed(random_state)\n","      sampled_indices = random.sample(range(len(lines)), sample_size)\n","      sampled_lines = [lines[i].strip() for i in sampled_indices]\n","      return sampled_lines\n","\n","def load_datasets(train_data_path, train_labels_path, test_data_path, test_labels_path, train_sample_size=25000, test_sample_size=10000):\n","    \"\"\"\n","    Load training and testing data and labels from specified file paths.\n","\n","    Args:\n","    train_data_path (str): Path to the training data file.\n","    train_labels_path (str): Path to the training labels file.\n","    test_data_path (str): Path to the testing data file.\n","    test_labels_path (str): Path to the testing labels file.\n","    train_sample_size (int): Number of samples to load from the training data.\n","    test_sample_size (int): Number of samples to load from the testing data.\n","\n","    Returns:\n","    tuple: Tuple containing loaded training data, training labels, testing data, testing labels.\n","    \"\"\"\n","    # Load training data and labels\n","    X_train = load_data_sample(train_data_path, sample_size=train_sample_size)\n","    y_train = load_data_sample(train_labels_path, sample_size=train_sample_size)\n","\n","    # Load testing data and labels\n","    X_test = load_data_sample(test_data_path, sample_size=test_sample_size)\n","    y_test = load_data_sample(test_labels_path, sample_size=test_sample_size)\n","\n","    return X_train, y_train, X_test, y_test\n","\n","\n","# Define the base path for data files\n","base_path = '/content/drive/MyDrive/data'\n","\n","# Paths to data files using the base path\n","train_data_path = f'{base_path}/train/x_train.txt'\n","train_labels_path = f'{base_path}/train/y_train.txt'\n","test_data_path = f'{base_path}/test/x_test.txt'\n","test_labels_path = f'{base_path}/test/y_test.txt'"],"metadata":{"id":"GQTyAk5FUm90","executionInfo":{"status":"ok","timestamp":1714914735553,"user_tz":-120,"elapsed":3,"user":{"displayName":"Rashed Sabra","userId":"05362189444872837845"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["X_train, y_train, X_test, y_test = load_datasets(train_data_path, train_labels_path, test_data_path, test_labels_path)"],"metadata":{"id":"y6yC0vOlU9BA","executionInfo":{"status":"ok","timestamp":1714914743767,"user_tz":-120,"elapsed":8216,"user":{"displayName":"Rashed Sabra","userId":"05362189444872837845"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Word2Vec Model Training and Transformation\n","\n","This section defines the Word2Vec transformation and training process. We utilize the `Word2Vec` model from the Gensim library, which is effective for natural language processing tasks such as embedding generation based on the context of words.\n","\n","### EpochLogger Callback\n","To monitor the training progress of the Word2Vec model, we implement the `EpochLogger` class as a callback. This callback logs the completion of each epoch during training, providing visibility into the model's training process. This is especially useful for long training sessions, as it gives real-time feedback about the progress.\n","\n","### Word2Vec Transformation Function\n","The `word2vec_transform` function is responsible for:\n","1. **Initializing and training the Word2Vec model**: The function takes sentences as input and trains a Word2Vec model. Key parameters such as `vector_size`, `window`, and `min_count` are configurable, allowing customization based on specific dataset characteristics.\n","2. **Generating word embeddings**: After training, the model computes the average Word2Vec embedding for each sentence. This average embedding represents the sentence in a dense vector form, which can be used for further machine learning tasks.\n","\n","This approach leverages the semantic richness of Word2Vec embeddings, providing a robust feature set for subsequent classification or clustering tasks.\n"],"metadata":{"id":"_qc4fBh_W7kf"}},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":283,"status":"ok","timestamp":1714919801538,"user":{"displayName":"Rashed Sabra","userId":"05362189444872837845"},"user_tz":-120},"id":"RRdJ2yLdyZBx"},"outputs":[],"source":["from gensim.models import Word2Vec\n","from gensim.models.callbacks import CallbackAny2Vec\n","\n","class EpochLogger(CallbackAny2Vec):\n","    def __init__(self):\n","        self.epoch = 0\n","    def on_epoch_end(self, model):\n","        print(f\"Epoch {self.epoch} completed\")\n","        self.epoch += 1\n","\n","def word2vec_transform(sentences, vector_size=100, window=5, min_count=1, epochs=10):\n","    model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, epochs=epochs)\n","    model.build_vocab(sentences, progress_per=1000)\n","    model.train(sentences, total_examples=model.corpus_total_words, epochs=epochs)\n","    return {word: model.wv[word] for word in model.wv.index_to_key}  # Dictionary of word vectors"]},{"cell_type":"markdown","metadata":{"id":"BD3mAKSMhAS4"},"source":["## Prediction and Model Training Functions\n","\n","This section outlines the core functionalities of our language identification pipeline, covering the model training, prediction, and evaluation processes. These functions are designed to work with different types of text vectorization techniques, namely TF-IDF and Word2Vec.\n","\n","### Predict Language Function\n","The `predict_language` function handles the prediction of the language for a given piece of text based on the specified model and vectorizer type:\n","- **TF-IDF Vectorization**: If the model is trained with TF-IDF, the prediction is straightforward—passing the text through the pipeline to get the predicted language.\n","- **Word2Vec Vectorization**: For Word2Vec, the text needs to be tokenized and transformed into Word2Vec embeddings before making a prediction. This involves converting the text into tokens, transforming these tokens into embeddings, and then reshaping the result to match the expected input structure for the classifier.\n","\n","### Train and Tune Model Function\n","The `train_and_tune_model` function configures and trains the model based on the specified vectorization technique:\n","- **TF-IDF**: A pipeline is set up with a TF-IDF vectorizer and a logistic regression classifier. Hyperparameters are tuned using GridSearchCV to find the best model configuration.\n","- **Word2Vec**: The training data are first transformed into Word2Vec embeddings, followed by training a logistic regression classifier. This function outputs the trained model ready for making predictions.\n","\n","### Predict and Evaluate Function\n","The `predict_and_evaluate` function is used to assess the performance of the trained model. It iterates over a dataset, applies the `predict_language` function to each text entry, and collects predictions. It then calculates and prints the accuracy of the model based on these predictions, providing a straightforward metric to evaluate the model's effectiveness.\n","\n","This setup allows for a flexible application of different text vectorization techniques, facilitating easy comparison and selection based on performance metrics.\n"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":600,"status":"ok","timestamp":1714919803767,"user":{"displayName":"Rashed Sabra","userId":"05362189444872837845"},"user_tz":-120},"id":"1KZAwSByg79F"},"outputs":[],"source":["import random\n","import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import make_pipeline, Pipeline\n","from sklearn.model_selection import GridSearchCV, train_test_split\n","from sklearn.metrics import accuracy_score\n","from gensim.models import Word2Vec\n","\n","\n","# Adjust in predict_language:\n","def predict_language(text, model, vectorizer_type='word2vec'):\n","    if vectorizer_type == 'word2vec':\n","        tokenized_text = word_tokenize(text.lower())\n","        transformed_text = np.mean([word2vec[word] for word in tokenized_text if word in word2vec], axis=0, keepdims=True)\n","        return model.predict(transformed_text)[0]\n","\n","def train_and_tune_model(X_train, y_train, vectorizer_type='tfidf'):\n","    if vectorizer_type == 'tfidf':\n","        vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(3, 3))\n","        pipeline = make_pipeline(vectorizer, LogisticRegression(max_iter=500))\n","        parameters = {'logisticregression__C': [0.1, 1], 'tfidfvectorizer__ngram_range': [(2, 2)]}\n","        grid_search = GridSearchCV(pipeline, parameters, cv=5, verbose=2)\n","        grid_search.fit(X_train, y_train)\n","        print(\"Best TF-IDF model selected.\")\n","        return grid_search.best_estimator_\n","    elif vectorizer_type == 'word2vec':\n","        print(\"Transforming training data using Word2Vec...\")\n","        X_train_transformed = word2vec_transform(X_train)\n","        pipeline = Pipeline([('classifier', LogisticRegression(max_iter=10000))])\n","        pipeline.fit(X_train_transformed, y_train)\n","        print(\"Word2Vec model fitted.\")\n","        return pipeline\n","\n","def predict_and_evaluate(X, y, model, vectorizer_type):\n","    predictions = []\n","    for index, text in enumerate(X):\n","        predictions.append(predict_language(text, model, vectorizer_type))\n","        if (index + 1) % 100 == 0 or index + 1 == len(X):\n","            print(f\"Completed {index + 1}/{len(X)} predictions\")\n","    accuracy = accuracy_score(y, predictions)\n","    print(f\"{vectorizer_type} Accuracy: {accuracy}\")\n","    return accuracy"]},{"cell_type":"markdown","source":["## Training, Evaluating, and Saving the TF-IDF Model\n","\n","This section is dedicated to handling the operations for the TF-IDF vectorized model. We follow a structured approach to train the model, evaluate its performance, and then save it for future use. This process ensures that we have a deployable model at hand without the need to retrain.\n","\n","### Training the Model\n","We start by training our model using the `train_and_tune_model` function with the TF-IDF vectorization approach. This function not only trains the model but also tunes its hyperparameters using GridSearchCV to find the optimal model settings. This ensures that our model performs at its best.\n","\n","### Evaluating the Model\n","Once the model is trained, we use the `predict_and_evaluate` function to test the model's performance on a separate test dataset. This function predicts the languages of the texts in the test dataset and calculates the accuracy, providing a quantitative measure of the model's effectiveness.\n","\n"],"metadata":{"id":"Dqp84w25XoAu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GD4wpGFnyy-Z"},"outputs":[],"source":["model_tfidf = train_and_tune_model(X_train, y_train, vectorizer_type='tfidf')\n","y_pred_tfidf = predict_and_evaluate(X_test, y_test, model_tfidf, 'tfidf')\n","# dump(model_tfidf, 'model_tfidf.joblib')\n","# model:lr_tfidf_bigram_best, ngram_range=(2, 2), iter = 500, tfidf, max_iter=500, Accuracy: 0.9274"]},{"cell_type":"code","source":["from google.colab import files\n","files.download('model_tfidf.joblib')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"khPENtEu8dvq","executionInfo":{"status":"ok","timestamp":1714905687086,"user_tz":-120,"elapsed":305,"user":{"displayName":"Rashed Sabra","userId":"05362189444872837845"}},"outputId":"ed6d9ac4-6f37-46d9-8a33-a9ffddfabc4b"},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_892718cb-304e-4ec8-8581-46372e27d274\", \"model_tfidf.joblib\", 240440625)"]},"metadata":{}}]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1166,"status":"ok","timestamp":1714905705527,"user":{"displayName":"Rashed Sabra","userId":"05362189444872837845"},"user_tz":-120},"id":"iR7wker3hItv","outputId":"99023249-ad42-4aac-f686-18dcf3de8d3a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Pipeline(steps=[('tfidfvectorizer',\n","                 TfidfVectorizer(analyzer='char', ngram_range=(2, 2))),\n","                ('logisticregression', LogisticRegression(C=1, max_iter=500))])\n"]}],"source":["from joblib import dump, load\n","loaded_model_tfidf = load('model_tfidf.joblib')\n","print(loaded_model_tfidf)"]},{"cell_type":"markdown","source":["## Training, Evaluating, and Saving the Word2Vec Model\n","\n","This section focuses on the procedures using the Word2Vec model. We demonstrate the comprehensive steps from training the model with Word2Vec embeddings, evaluating its performance on the test data, and finally saving the model for later use. These steps ensure our model is both effective and reusable.\n","\n","### Training the Word2Vec Model\n","The `train_and_tune_model` function is utilized here with the `word2vec` vectorization type. Unlike traditional TF-IDF, Word2Vec provides a dense representation of words which captures semantic relationships. This function trains the model using these embeddings and integrates them into a logistic regression framework to predict the languages. It's designed to offer a deep understanding of context within the text data.\n","\n","### Evaluating the Model\n","After training, the performance of the Word2Vec model is evaluated using the `predict_and_evaluate` function. This function applies the trained model to the test dataset to predict languages, and calculates the accuracy to quantify how well our model performs in real-world scenarios. This step is crucial for validating the effectiveness of the Word2Vec embeddings in the task of language identification.\n"],"metadata":{"id":"e6lEcIPlX4qG"}},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1714926049465,"user":{"displayName":"Rashed Sabra","userId":"05362189444872837845"},"user_tz":-120},"id":"I4IumQZ2zUUf"},"outputs":[],"source":["# Call the modified function for predictions and evaluations\n","model_word2vec = train_and_tune_model(X_train, y_train, vectorizer_type='word2vec')\n","y_pred_word2vec = predict_and_evaluate(X_test, y_test, model_word2vec, 'word2vec')"]},{"cell_type":"code","source":["# dump(model_word2vec, 'model_word2vec.joblib')\n","files.download('model_word2vec.joblib')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"jdSljs1kM6mt","executionInfo":{"status":"ok","timestamp":1714909286134,"user_tz":-120,"elapsed":340,"user":{"displayName":"Rashed Sabra","userId":"05362189444872837845"}},"outputId":"a420eca0-ec0b-4ac3-8a49-c3e1c55981a7"},"execution_count":70,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_ef54d4db-6c46-4182-a9c8-d3aad90c9bed\", \"model_word2vec.joblib\", 199253)"]},"metadata":{}}]},{"cell_type":"markdown","source":["# Conclusion\n","\n","In our exploration of the second approach using Word2Vec embeddings and hyperparameter optimization via GridSearch, we have gained significant insights into the nuances of language identification. This method, while offering a deeper understanding of contextual relationships within text data, presented several challenges:\n","\n","- **Model Complexity and Training Time**: The Word2Vec model requires substantial computational resources and time for training, especially as the size of the dataset increases.\n","- **Convergence Issues**: Despite hyperparameter tuning, logistic regression sometimes struggled to converge, indicating potential limitations in handling high-dimensional data effectively.\n","\n","These observations underscore the need for alternative approaches that can leverage existing linguistic knowledge and reduce the dependency on extensive training regimes.\n"],"metadata":{"id":"sOuOue3j-jDC"}},{"cell_type":"code","source":[],"metadata":{"id":"GoHe2_j3kM9L"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyPshI237Hb8FUln4mSSiZis"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}