{"cells":[{"cell_type":"markdown","source":["## Introduction to Approach 3:Fine-Tuning BERT for Language Identification\n","\n","In this notebook, I worked on an advanced approach to language identification by fine-tuning a pre-trained BERT (Bidirectional Encoder Representations from Transformers) model. My objective is to leverage a multi-lingual BERT model, specifically designed to handle text in multiple languages, to accurately identify the language of given text samples.\n","\n","### Why BERT?\n","BERT models are pre-trained on a large corpus of text from numerous sources, enabling them to capture a rich language structure. By fine-tuning BERT on our specific task of language identification, I aim to harness its powerful language representations to improve classification accuracy across diverse languages, even with relatively minimal additional training.\n","\n","### Goals and Setup\n","The goal is to fine-tune the `bert-base-multilingual-cased` model, which supports 104 languages and has been cased to preserve the case of the input text, enhancing its ability to recognize language-specific nuances better.\n","\n","will:\n","- Set up the model and tokenizer for multilingual input.\n","- Prepare the dataset, which includes encoding texts and converting language labels to a format suitable for BERT.\n","- Define training parameters that balance efficiency with performance, allowing for quick iterations and adjustments.\n","- Utilize the Hugging Face Transformers library to streamline model training and evaluation.\n","\n","References:\n","\n","\n","1. Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. *arXiv preprint arXiv:1810.04805*. Available at https://arxiv.org/abs/1810.04805\n","\n","2. Google AI Blog. (2020). Announcing Multilingual BERT. Available at https://ai.googleblog.com/2020/08/language-agnostic-bert-sentence.html\n","\n","3. Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). Attention Is All You Need. *arXiv preprint arXiv:1706.03762*. Available at https://arxiv.org/abs/1706.03762\n","\n","4. Hugging Face. Transformers: State-of-the-art Natural Language Processing. Software available from https://huggingface.co/transformers/\n"],"metadata":{"id":"5XsmW_1pX0Ev"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"zQvGV5zpbNBR","executionInfo":{"status":"ok","timestamp":1714943373986,"user_tz":-120,"elapsed":12957,"user":{"displayName":"Rashed Sabra","userId":"05362189444872837845"}}},"outputs":[],"source":["import numpy as np\n","import random\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from transformers import Trainer, TrainingArguments\n","from transformers import AutoTokenizer\n","import torch\n","from torch.utils.data import Dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"6WHvxevirDK5","executionInfo":{"status":"ok","timestamp":1714943373987,"user_tz":-120,"elapsed":17,"user":{"displayName":"Rashed Sabra","userId":"05362189444872837845"}}},"outputs":[],"source":["# !pip install transformers[torch]"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"0Ty2oWKzdvex","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714943375740,"user_tz":-120,"elapsed":1768,"user":{"displayName":"Rashed Sabra","userId":"05362189444872837845"}},"outputId":"e6b81597-2bc7-44ff-865a-dd59a7b42e36"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["# Data Loading and Sampling\n","This section covers the loading and random sampling of the training and testing data. Sampling is particularly useful to reduce computation time while developing the model on Google Colab. We use a sample size of 25,000 for training and 10,000 for testing to ensure a representative subset of the full dataset."],"metadata":{"id":"Fx9EiuP5T7rB"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"oAG6u0Q3l-DO","executionInfo":{"status":"ok","timestamp":1714931920300,"user_tz":-120,"elapsed":3,"user":{"displayName":"Rashed Sabra","userId":"05362189444872837845"}}},"outputs":[],"source":["def load_data_sample(filepath, sample_size=10, random_state=42):\n","    \"\"\"\n","    Load a random sample of lines from a file to reduce memory usage and speed up computations.\n","    \"\"\"\n","    with open(filepath, 'r', encoding='utf-8') as file:\n","      lines = file.readlines()\n","\n","    random.seed(random_state)\n","    sampled_indices = random.sample(range(len(lines)), sample_size)\n","    sampled_lines = [lines[i].strip() for i in sampled_indices]\n","    return sampled_lines\n","\n","def load_datasets(train_data_path, train_labels_path, test_data_path, test_labels_path, train_sample_size=25000, test_sample_size=10000):\n","    \"\"\"\n","    Load training and testing data and labels from specified file paths.\n","\n","    Args:\n","    train_data_path (str): Path to the training data file.\n","    train_labels_path (str): Path to the training labels file.\n","    test_data_path (str): Path to the testing data file.\n","    test_labels_path (str): Path to the testing labels file.\n","    train_sample_size (int): Number of samples to load from the training data.\n","    test_sample_size (int): Number of samples to load from the testing data.\n","\n","    Returns:\n","    tuple: Tuple containing loaded training data, training labels, testing data, testing labels.\n","    \"\"\"\n","    # Load training data and labels\n","    X_train = load_data_sample(train_data_path, sample_size=train_sample_size)\n","    y_train = load_data_sample(train_labels_path, sample_size=train_sample_size)\n","\n","    # Load testing data and labels\n","    X_test = load_data_sample(test_data_path, sample_size=test_sample_size)\n","    y_test = load_data_sample(test_labels_path, sample_size=test_sample_size)\n","\n","    return X_train, y_train, X_test, y_test\n","\n","\n","# Define the base path for data files\n","base_path = '/content/drive/MyDrive/data'\n","\n","# Paths to data files using the base path\n","train_data_path = f'{base_path}/train/x_train.txt'\n","train_labels_path = f'{base_path}/train/y_train.txt'\n","test_data_path = f'{base_path}/test/x_test.txt'\n","test_labels_path = f'{base_path}/test/y_test.txt'"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"75D9EMYOpNjr","executionInfo":{"status":"ok","timestamp":1714931920834,"user_tz":-120,"elapsed":537,"user":{"displayName":"Rashed Sabra","userId":"05362189444872837845"}}},"outputs":[],"source":["# Load all datasets using simplified paths\n","X_train, y_train, X_test, y_test = load_datasets(train_data_path, train_labels_path, test_data_path, test_labels_path)"]},{"cell_type":"code","execution_count":69,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":301,"status":"ok","timestamp":1714938803362,"user":{"displayName":"Rashed Sabra","userId":"05362189444872837845"},"user_tz":-120},"id":"F1w1ht-ZnBsI","outputId":"7ae24164-f4b6-4805-e79d-1e1962e4f6f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["By the late 1980s, CBS was telecasting 15 or 16 regular season games per year. In 1989 alone, only 13 of the 24 playoff games (Games 1–3, specifically) in Round 1 aired on TBS or CBS (for example, none of the four games from the Seattle–Houston first round series appeared on national television). Notably, Game 5 of the 1989 playoff series between the Chicago Bulls and Cleveland Cavaliers (featuring Michael Jordan's now famous game winning, last second shot over Craig Ehlo) was not nationally televised. CBS affiliates in Virginia elected to show the first game of a second round series between Seattle and the Lakers.\n","cbk\n"]}],"source":["print(X_train[0])\n","print(y_train[0])"]},{"cell_type":"markdown","metadata":{"id":"FfSkT3cIoW9Z"},"source":["initialize and load the pre-trained BERT model along with its tokenizer, configured specifically for our multilingual language classification task involving 235 unique labels (according to our dataset).\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g3CrME1l6EH_"},"outputs":[],"source":["# Load tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","model = BertForSequenceClassification.from_pretrained(\n","    'bert-base-multilingual-cased',\n","    num_labels=235,  # The number of unique labels in the dataset\n","    output_attentions = False,\n","    output_hidden_states = False,\n",")"]},{"cell_type":"markdown","metadata":{"id":"K3SD2R3coR5n"},"source":["Encode Texts and Labels"]},{"cell_type":"code","source":["def encode_texts(texts):\n","    return tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)"],"metadata":{"id":"e3BoWNqpJYq0","executionInfo":{"status":"ok","timestamp":1714935985725,"user_tz":-120,"elapsed":57105,"user":{"displayName":"Rashed Sabra","userId":"05362189444872837845"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# Combine all labels from both train and test sets to ensure all are known before encoding\n","all_labels = sorted(set(y_train) | set(y_test))  # Union of y_train and y_test labels, sorted for consistency\n","\n","# Create a mapping from language code to a unique index\n","lang2idx = {lang: idx for idx, lang in enumerate(all_labels)}\n","\n","# Function to encode labels based on the mapping\n","def encode_labels(labels):\n","    return np.array([lang2idx[lang] for lang in labels])"],"metadata":{"id":"dodCYGi-Jqg_","executionInfo":{"status":"ok","timestamp":1714936001338,"user_tz":-120,"elapsed":289,"user":{"displayName":"Rashed Sabra","userId":"05362189444872837845"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nmUPJY7ioOoY"},"source":["`LanguageDataset` class, which is responsible for organizing and preparing the encoded texts and labels for use with the BERT model.\n"]},{"cell_type":"code","source":["\n","class LanguageDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","# Assuming 'encode_texts' function from the previous example tokenizes and encodes texts\n","train_encodings = encode_texts(X_train)\n","test_encodings = encode_texts(X_test)\n","\n","# Assuming 'encode_labels' converts labels to integer indices\n","train_labels = encode_labels(y_train)\n","test_labels = encode_labels(y_test)\n","\n","# Create Dataset objects\n","train_dataset = LanguageDataset(train_encodings, train_labels)\n","eval_dataset = LanguageDataset(test_encodings, test_labels)\n"],"metadata":{"id":"WhOjQCBIJs0Q","executionInfo":{"status":"ok","timestamp":1714936128491,"user_tz":-120,"elapsed":56796,"user":{"displayName":"Rashed Sabra","userId":"05362189444872837845"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QcOjot2WocX1"},"source":["configure the training parameters for our model, specifying details such as the number of epochs, batch sizes for training and evaluation, and the directory for saving model checkpoints.\n"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"Q_q2RdRfOXlv","executionInfo":{"status":"ok","timestamp":1714936251833,"user_tz":-120,"elapsed":5,"user":{"displayName":"Rashed Sabra","userId":"05362189444872837845"}}},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir='./bert_base_model_results',  # directory to save model checkpoints\n","    num_train_epochs=2,            # to reduce computational load and time\n","    per_device_train_batch_size=16,  # batch size for training\n","    per_device_eval_batch_size=64,   # batch size for evaluation\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    evaluation_strategy=\"epoch\"\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset\n",")"]},{"cell_type":"code","source":["# Run with a smaller subset to test\n","small_train_dataset = LanguageDataset(train_encodings[:100], train_labels[:100])\n","small_eval_dataset = LanguageDataset(test_encodings[:50], test_labels[:50])\n","\n","test_trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=small_train_dataset,\n","    eval_dataset=small_eval_dataset\n",")\n","test_trainer.train()\n"],"metadata":{"id":"xgT9tTfeJi7z"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3JoEuYgzplL7"},"outputs":[],"source":["trainer.train()\n","# cell output:\n","# Epoch\tTraining Loss\tValidation Loss ---- [3126/3126 21:29, Epoch 2/2]\n","# 1\t0.065400\t0.286143\n","# 2\t0.064200\t0.259737"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wjU9uWjv89KE"},"outputs":[],"source":["model.save_pretrained('./final_model')\n","tokenizer.save_pretrained('./final_model')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pi8Z-H8U9hMj"},"outputs":[],"source":["evaluation_results = trainer.evaluate()\n","# cell output:\n","# {'eval_loss': 0.2597372531890869,\n","#  'eval_runtime': 66.2139,\n","#  'eval_samples_per_second': 151.026,\n","#  'eval_steps_per_second': 2.371,\n","#  'epoch': 2.0}"]},{"cell_type":"code","source":["from google.colab import files\n","\n","# Zip the model directory for easier download\n","!zip -r final_model.zip ./final_model\n","\n","# Trigger the browser download\n","files.download('final_model.zip')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139},"id":"blPgNUIObsHp","executionInfo":{"status":"ok","timestamp":1714940769969,"user_tz":-120,"elapsed":39831,"user":{"displayName":"Rashed Sabra","userId":"05362189444872837845"}},"outputId":"aabed54a-2463-4bea-9f3f-4282dccb6e49"},"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: final_model/ (stored 0%)\n","  adding: final_model/special_tokens_map.json (deflated 42%)\n","  adding: final_model/config.json (deflated 77%)\n","  adding: final_model/model.safetensors (deflated 7%)\n","  adding: final_model/tokenizer_config.json (deflated 75%)\n","  adding: final_model/vocab.txt (deflated 45%)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_8d6a05b5-9249-4d18-82ca-53d1bd02db9a\", \"final_model.zip\", 660890408)"]},"metadata":{}}]},{"cell_type":"code","source":["# # Zip the model directory for easier download\n","# !zip -r bert_base_model_results.zip ./bert_base_model_results\n","\n","# # Trigger the browser download\n","# files.download('bert_base_model_results.zip')\n","files.download('final_model')"],"metadata":{"id":"ZdQDbYV5c5OT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4MsHBXG_daqs"},"source":["# Testing the model"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"MPkJjKsWes-G","executionInfo":{"status":"ok","timestamp":1714943387454,"user_tz":-120,"elapsed":4041,"user":{"displayName":"Rashed Sabra","userId":"05362189444872837845"}}},"outputs":[],"source":["# Load the model and tokenizer\n","model_path = './final_model'\n","loaded_model = BertForSequenceClassification.from_pretrained(model_path)\n","loaded_tokenizer = BertTokenizer.from_pretrained(model_path)"]},{"cell_type":"code","source":["\n","# Create a sorted list of unique labels\n","unique_labels = sorted(set(y_train))\n","\n","# Create a mapping from labels to indices\n","label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n","\n","# Invert the mapping to get from indices to labels\n","index_to_language = {idx: label for label, idx in label_to_index.items()}\n"],"metadata":{"id":"3tsxPkaAQMSR","executionInfo":{"status":"ok","timestamp":1714943408955,"user_tz":-120,"elapsed":305,"user":{"displayName":"Rashed Sabra","userId":"05362189444872837845"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def predict(text, tokenizer, model, index_to_language):\n","    model.eval()  # Ensure the model is in evaluation mode\n","    inputs = encode_texts(text, tokenizer)  # Encode the input text\n","    with torch.no_grad():  # Disable gradient calculation for inference\n","        outputs = model(**inputs)  # Forward pass\n","        logits = outputs.logits\n","        prediction_index = torch.argmax(logits, dim=-1).item()  # Get the index of the highest logit\n","\n","    # Convert index to language using the mapping\n","    predicted_language = index_to_language[prediction_index]\n","    return predicted_language"],"metadata":{"id":"JZlZOKTeRHD5","executionInfo":{"status":"ok","timestamp":1714939489431,"user_tz":-120,"elapsed":314,"user":{"displayName":"Rashed Sabra","userId":"05362189444872837845"}}},"execution_count":74,"outputs":[]},{"cell_type":"code","source":["# Example English text\n","english_text = \"This is a simple test to see how the BERT model classifies this sentence.\"\n","\n","# Predict using the pretrained model\n","predicted_language = predict(english_text, loaded_tokenizer, loaded_model, index_to_language)\n","\n","print(\"Predicted Language:\", predicted_language)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mMsY7SOjQ-0X","executionInfo":{"status":"ok","timestamp":1714939490918,"user_tz":-120,"elapsed":294,"user":{"displayName":"Rashed Sabra","userId":"05362189444872837845"}},"outputId":"f3711de7-e91c-4561-9f7f-5c928a6922ae"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted Language: eng\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"mount_file_id":"1PY8lRWag5I0p629ffDQ0OF3zuhhNbd7H","authorship_tag":"ABX9TyPeLYUVdgsNfX1Rk5TQbR/r"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}