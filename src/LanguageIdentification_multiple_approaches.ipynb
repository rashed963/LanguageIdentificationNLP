{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rashed963/LanguageIdentificationNLP/blob/main/LanguageIdentification_multiple_approaches.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zQvGV5zpbNBR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import log_loss\n",
        "import random\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ty2oWKzdvex",
        "outputId": "bc04c5cd-531a-4b2e-a7a8-d9428d7cd21a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "63QyGaD4dkYK"
      },
      "outputs": [],
      "source": [
        "def load_data_sample(filepath, sample_size=10, random_state=42):\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    random.seed(random_state)\n",
        "    sampled_indices = random.sample(range(len(lines)), sample_size)\n",
        "    sampled_lines = [lines[i].strip() for i in sampled_indices]\n",
        "    return sampled_lines\n",
        "\n",
        "def train_model(X_train, y_train):\n",
        "    model = make_pipeline(CountVectorizer(analyzer='char', ngram_range=(3, 3)), LogisticRegression(max_iter=1000))\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "def predict_language(text, model):\n",
        "    return model.predict([text])[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Gt92c535KWX7"
      },
      "outputs": [],
      "source": [
        "filepath = '/content/drive/MyDrive/data/train/x_train.txt'\n",
        "X_train = load_data_sample(filepath,sample_size=10000)\n",
        "filepath = '/content/drive/MyDrive/data/train/y_train.txt'\n",
        "y_train = load_data_sample(filepath,sample_size=10000)\n",
        "filepath = '/content/drive/MyDrive/data/test/x_test.txt'\n",
        "X_test = load_data_sample(filepath,sample_size=10000)\n",
        "filepath = '/content/drive/MyDrive/data/test/y_test.txt'\n",
        "y_test = load_data_sample(filepath,sample_size=10000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3VG49z65V67"
      },
      "source": [
        "**V1. Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1yVsogGdmsr",
        "outputId": "a552ec3a-3dc0-459e-b5d6-0a40bd7672de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.17\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Train the model\n",
        "model = train_model(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = [predict_language(text, model) for text in X_test]\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XauPnD_pu8B"
      },
      "source": [
        "**V2. search for the best hyberparams values**\n",
        "\n",
        "Enhance the Pipeline: Your existing pipeline uses a count vectorizer and logistic regression. You can add hyperparameter options to both.\n",
        "Setup GridSearchCV: Define the parameter grid that GridSearchCV will explore.\n",
        "Cross-validation Setup: Choose a suitable cross-validation strategy, typically k-fold cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "d_g_fJm5qBuR"
      },
      "outputs": [],
      "source": [
        "def train_and_tune_model(X_train, y_train):\n",
        "    pipeline = make_pipeline(\n",
        "        CountVectorizer(analyzer='char', ngram_range=(3, 3)),\n",
        "        LogisticRegression(max_iter=10)\n",
        "    )\n",
        "    parameters = {\n",
        "        # 'countvectorizer__ngram_range': [(1,1), (1,2), (2,2), (2,3), (3,3)],\n",
        "        # 'logisticregression__C': [0.01, 0.1, 1, 10, 100]\n",
        "        'countvectorizer__ngram_range': [(1,1)],\n",
        "        'logisticregression__C': [1]\n",
        "    }\n",
        "    grid_search = GridSearchCV(pipeline, parameters, cv=3, verbose=1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    return grid_search.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZkNuYJ_qLfb"
      },
      "outputs": [],
      "source": [
        "# Train the model with hyperparameter tuning\n",
        "model = train_and_tune_model(X_train, y_train)\n",
        "print(model.decision_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PPBgWVIqPEk",
        "outputId": "4cdec67c-5104-476e-bf91-f3b327175d2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.235\n"
          ]
        }
      ],
      "source": [
        "# Predict and evaluate\n",
        "y_pred = [predict_language(text, model) for text in X_test]\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8-3Mgpws3UH"
      },
      "source": [
        "**V2. Exploring different vectorization and feature extraction techniques for language identification**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14wm7kxKtQj6",
        "outputId": "a75e60b9-f120-4724-e823-d7aa18a148b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n",
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Accuracy: 0.035\n",
            "Word2Vec Accuracy: 0.0\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline, Pipeline\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "def predict_language(text, model, vectorizer_type='tfidf'):\n",
        "    if vectorizer_type == 'tfidf':\n",
        "        # For TF-IDF, predict directly using the model\n",
        "        return model.predict([text])[0]\n",
        "    elif vectorizer_type == 'word2vec':\n",
        "        # For Word2Vec, transform the text first\n",
        "        # Tokenize the single text\n",
        "        tokenized_text = word_tokenize(text)\n",
        "        # Transform the text using Word2Vec\n",
        "        transformed_text = word2vec_transform([tokenized_text])\n",
        "        # Reshape the input to (1, -1), which is (1 sample, N features)\n",
        "        transformed_text = transformed_text.reshape(1, -1)\n",
        "        return model.predict(transformed_text)[0]\n",
        "\n",
        "# Function to train Word2Vec and transform data\n",
        "def word2vec_transform(sentences, vector_size=100, window=5, min_count=1, epochs=10):\n",
        "    # Initialize and train a Word2Vec model\n",
        "    model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, epochs=epochs)\n",
        "    word_vectors = model.wv\n",
        "    # Compute the average of word vectors for each sentence\n",
        "    return np.array([\n",
        "        np.mean([word_vectors[w] for w in words if w in word_vectors.key_to_index] or [np.zeros(vector_size)], axis=0)\n",
        "        for words in sentences\n",
        "    ])\n",
        "\n",
        "# Define the model and hyperparameters\n",
        "def train_and_tune_model(X_train, y_train, vectorizer_type='tfidf'):\n",
        "    if vectorizer_type == 'tfidf':\n",
        "        vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(3, 3))\n",
        "        pipeline = make_pipeline(vectorizer, LogisticRegression(max_iter=1000))\n",
        "    elif vectorizer_type == 'word2vec':\n",
        "        X_train_transformed = word2vec_transform(X_train)\n",
        "        pipeline = Pipeline([('classifier', LogisticRegression(max_iter=1000))])\n",
        "        pipeline.fit(X_train_transformed, y_train)\n",
        "        return pipeline\n",
        "\n",
        "    parameters = {'logisticregression__C': [1]}  # Simplified for this example\n",
        "    if vectorizer_type == 'tfidf':\n",
        "        parameters['tfidfvectorizer__ngram_range'] = [(1,1)]  # Simplified for this example\n",
        "\n",
        "    if vectorizer_type != 'word2vec':\n",
        "        grid_search = GridSearchCV(pipeline, parameters, cv=3, verbose=1)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        return grid_search.best_estimator_\n",
        "\n",
        "\n",
        "model_tfidf = train_and_tune_model(X_train, y_train, vectorizer_type='tfidf')\n",
        "model_word2vec = train_and_tune_model(X_train, y_train, vectorizer_type='word2vec')\n",
        "\n",
        "y_pred_tfidf = [predict_language(text, model_tfidf, 'tfidf') for text in X_test]\n",
        "print(\"TF-IDF Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n",
        "\n",
        "y_pred_word2vec = [predict_language(text, model_word2vec, 'word2vec') for text in X_test]\n",
        "print(\"Word2Vec Accuracy:\", accuracy_score(y_test, y_pred_word2vec))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKzrsqvy2PT9"
      },
      "source": [
        "**V3. **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "k-pPTRmF2SNS"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import AutoTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "g3CrME1l6EH_"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qd1uNkkn6ioI"
      },
      "outputs": [],
      "source": [
        "def encode_texts(texts):\n",
        "    return tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
        "\n",
        "X_train_enc = encode_texts(X_train)\n",
        "X_test_enc = encode_texts(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "FvrG3nWP68uN"
      },
      "outputs": [],
      "source": [
        "# Combine all labels from both train and test sets to ensure all are known before encoding\n",
        "all_labels = sorted(set(y_train) | set(y_test))  # Union of y_train and y_test labels, sorted for consistency\n",
        "\n",
        "# Create a mapping from language code to a unique index\n",
        "lang2idx = {lang: idx for idx, lang in enumerate(all_labels)}\n",
        "\n",
        "# Function to encode labels based on the mapping\n",
        "def encode_labels(labels):\n",
        "    return np.array([lang2idx[lang] for lang in labels])\n",
        "\n",
        "# Encode both training and testing labels\n",
        "y_train_enc = encode_labels(y_train)\n",
        "y_test_enc = encode_labels(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEn7ItXd7t_f",
        "outputId": "92e372db-c4f8-48da-a1e1-dd5848be9e42"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "num_labels = 235\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=num_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "cuI2v5iI9Nqy"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class LanguageDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Assuming 'encode_texts' function from the previous example tokenizes and encodes texts\n",
        "train_encodings = encode_texts(X_train)\n",
        "test_encodings = encode_texts(X_test)\n",
        "\n",
        "# Assuming 'encode_labels' converts labels to integer indices\n",
        "train_labels = encode_labels(y_train)\n",
        "test_labels = encode_labels(y_test)\n",
        "\n",
        "# Create Dataset objects\n",
        "train_dataset = LanguageDataset(train_encodings, train_labels)\n",
        "eval_dataset = LanguageDataset(test_encodings, test_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "AwYJzIa58BLN",
        "outputId": "9191be9a-7c75-455c-8c4d-f75a4e15f0f7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-23-8b59c98b5194>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='611' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 611/1875 03:47 < 07:51, 2.68 it/s, Epoch 0.98/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-23-8b59c98b5194>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./bert_base_model_results',  # directory to save model checkpoints\n",
        "    num_train_epochs=3,            # number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size for training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,               # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,              # weight decay for regularization\n",
        "    logging_dir='./logs',           # directory for storing logs\n",
        "    evaluation_strategy=\"epoch\"     # evaluate the model at the end of each epoch\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,  # training dataset\n",
        "    eval_dataset=eval_dataset     # evaluation dataset\n",
        ")\n",
        "\n",
        "trainer.train()  # Start training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjU9uWjv89KE"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained('./final_model')  # Save the fine-tuned model\n",
        "tokenizer.save_pretrained('./final_model')  # Save the tokenizer used with the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pi8Z-H8U9hMj"
      },
      "outputs": [],
      "source": [
        "evaluation_results = trainer.evaluate()\n",
        "\n",
        "evaluation_results\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNtVYZ4pMcOZSO9yknEoQzS",
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
